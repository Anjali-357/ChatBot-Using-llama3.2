ğŸš€ Features
ğŸ§  Context-aware multi-turn conversation
ğŸ”„ Role-based message structuring (system, user, assistant)
âš™ï¸ Dynamic response control (temperature tuning)
ğŸ— Modular architecture (Input â†’ Memory â†’ LLM â†’ Response)
ğŸ’» Fully local inference (no API keys required)
ğŸ›  Tech Stack
Python
LLaMA 3.2 (3B)
Ollama
Gradio
ğŸ— Architecture
User Input
   â†“
Memory Manager
   â†“
Message Builder (role-based)
   â†“
Ollama (LLaMA 3.2)
   â†“
Formatted Response
Uses:
ollama.chat(model="llama3.2:3b", messages=messages)
for structured multi-turn reasoning.
ğŸ“¦ Setup
1ï¸âƒ£ Install Ollama & Model
ollama pull llama3.2:3b
2ï¸âƒ£ Create Environment
conda create -n llm_env python=3.10 -y
conda activate llm_env
3ï¸âƒ£ Install Dependencies
pip install gradio ollama
â–¶ï¸ Run
python -m src.chatbot
Open in browser:
http://127.0.0.1:7860
ğŸ”® Future Improvements
Streaming responses
Vector database (RAG) integration
Tool-calling agent framework
Cloud deployment
